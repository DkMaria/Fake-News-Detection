{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transe_no2.ipynb",
      "provenance": [],
      "mount_file_id": "176eSK560a9e2uezImm5eMPE0tNcO2v_7",
      "authorship_tag": "ABX9TyOo+B1nBVhc4Ps/6SF16NXx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DkMaria/Fake-News-Detection/blob/main/transe_no2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KGE1401 with no the information of label"
      ],
      "metadata": {
        "id": "No5XGAI305M4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJNuY8aG04sy",
        "outputId": "8e11424c-5474-471e-c5f1-5f868b98dbed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.7.0\n",
            "Uninstalling tensorflow-2.7.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow-2.7.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "y\n",
            "  Successfully uninstalled tensorflow-2.7.0\n",
            "y\n"
          ]
        }
      ],
      "source": [
        "pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install \"tensorflow>=1.15.2,<2.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Fpv59U7l1CHf",
        "outputId": "c871ebd3-5fa1-4005-e6d9-d9ce01c322ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow<2.0,>=1.15.2\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 989 bytes/s \n",
            "\u001b[?25hCollecting h5py<=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 41.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 23.1 MB/s \n",
            "\u001b[?25hCollecting numpy<1.19.0,>=1.16.0\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.5 MB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 28.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (0.37.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.0,>=1.15.2) (1.13.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (3.3.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (4.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2.0,>=1.15.2) (3.10.0.2)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=dbf4345a799d507f913bb9bd405d208c7401c6158d18fd2bd8a6221c2c2ef033\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.7 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 numpy-1.18.5 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install AmpliGraph and other dependencies\n",
        "\n",
        "%%capture \n",
        "# Install AmpliGraph library\n",
        "! pip install ampligraph\n",
        "\n",
        "# Required to visualize embeddings with tensorboard projector, comment out if not required!\n",
        "! pip install --user tensorboard\n",
        "\n",
        "# Required to plot text on embedding clusters, comment out if not required!\n",
        "! pip install --user git+https://github.com/Phlya/adjustText"
      ],
      "metadata": {
        "id": "pRJ01iPy1FN1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if tensorflow is correctly installed \n",
        "import tensorflow as tf \n",
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHcKq1rw1Fs-",
        "outputId": "67761065-d154-4337-ea60-16cb44f21d78"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.15.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports used in this notebook\n",
        "%tensorflow_version 1.x\n",
        "import ampligraph\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "from ampligraph.datasets import load_from_csv \n",
        "from ampligraph.evaluation import train_test_split_no_unseen, evaluate_performance, mr_score, mrr_score, hits_at_n_score\n",
        "from ampligraph.latent_features import TransE\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from ampligraph.utils import create_tensorboard_visualizations "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcpLfRXk1HVB",
        "outputId": "33d8e184-b6cb-47f6-a39c-9954f183f4b6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow is already loaded. Please restart the runtime to change versions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "true = load_from_csv('.', '/content/drive/MyDrive/results15.csv', sep=',') \n",
        "false = load_from_csv('.', '/content/drive/MyDrive/resultsf15.csv', sep = ',')\n",
        "\n",
        "df_true = pd.DataFrame(true, columns = ['id', 'text', 'ratingName', 'author',  'headline', 'named_entities_claim', 'named_entities_article', 'keywords', 'source', 'sourceURL', 'link', 'language'])\n",
        "df_false = pd.DataFrame(false, columns = ['id', 'text', 'ratingName', 'author', 'headline', 'named_entities_claim', 'named_entities_article', 'keywords', 'source', 'sourceURL', 'link', 'language'])\n",
        "\n",
        "df_true['label'] = 0\n",
        "df_false['label'] = 1\n",
        "\n",
        "data = pd.concat([df_false, df_true])\n",
        "# shuffle the dataset\n",
        "data = data.sample(frac=1)\n",
        "\n",
        "# drop data with no additional information\n",
        "data = data.drop(columns= 'ratingName')\n",
        "data = data.drop(columns= 'sourceURL')\n",
        "data = data.drop(columns = 'link')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "Tvmlye-21LW7",
        "outputId": "ecdaf2ad-81e7-4439-8147-24179875c99f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b054b00f-4148-4741-92a9-edb1a2f10a17\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "      <th>headline</th>\n",
              "      <th>named_entities_claim</th>\n",
              "      <th>named_entities_article</th>\n",
              "      <th>keywords</th>\n",
              "      <th>source</th>\n",
              "      <th>language</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5752</th>\n",
              "      <td>http://data.gesis.org/claimskg/claim_review/7b...</td>\n",
              "      <td>Former NFL quarterback Tim Tebow knelt on the ...</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>Did Tim Tebow Kneel During the National Anthem...</td>\n",
              "      <td>2009 BCS Championship Game,American national a...</td>\n",
              "      <td>Tim Tebow,quarterback</td>\n",
              "      <td>NaN</td>\n",
              "      <td>snopes</td>\n",
              "      <td>English</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1971</th>\n",
              "      <td>http://data.gesis.org/claimskg/claim_review/0e...</td>\n",
              "      <td>'Trump Management was charged with discriminat...</td>\n",
              "      <td>Hillary Clinton</td>\n",
              "      <td>Clinton ad: Trump Management was charged with ...</td>\n",
              "      <td>Art of the Deal,Civil Rights Act of 1968,Donal...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Candidate Biography,Civil Rights,Housing,Legal...</td>\n",
              "      <td>politifact</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5597</th>\n",
              "      <td>http://data.gesis.org/claimskg/claim_review/90...</td>\n",
              "      <td>'...the earth moves closer to the sun every ye...</td>\n",
              "      <td>Scott Wagner</td>\n",
              "      <td>PA GOPer’s climate change theory debunked: Nop...</td>\n",
              "      <td>Drexel University,NPR,Pennsylvania Democratic ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Climate Change</td>\n",
              "      <td>politifact</td>\n",
              "      <td>English</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2356</th>\n",
              "      <td>http://data.gesis.org/claimskg/claim_review/b0...</td>\n",
              "      <td>Says 'in this next biennium, the cost of prima...</td>\n",
              "      <td>John Kitzhaber</td>\n",
              "      <td>Are increased PERS costs responsible for a $50...</td>\n",
              "      <td>2007 stock market crash,John Kitzhaber,Oregon ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Education</td>\n",
              "      <td>politifact</td>\n",
              "      <td>English</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7790</th>\n",
              "      <td>http://data.gesis.org/claimskg/claim_review/a4...</td>\n",
              "      <td>'They didn’t even put up a fight against SALT'</td>\n",
              "      <td>Donald Trump</td>\n",
              "      <td>Trump says NY didn't fight SALT cap</td>\n",
              "      <td>Adirondack,Albany,Andrew Cuomo,Catskills,Charl...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Taxes</td>\n",
              "      <td>politifact</td>\n",
              "      <td>English</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b054b00f-4148-4741-92a9-edb1a2f10a17')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b054b00f-4148-4741-92a9-edb1a2f10a17 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b054b00f-4148-4741-92a9-edb1a2f10a17');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                     id  ... label\n",
              "5752  http://data.gesis.org/claimskg/claim_review/7b...  ...     1\n",
              "1971  http://data.gesis.org/claimskg/claim_review/0e...  ...     0\n",
              "5597  http://data.gesis.org/claimskg/claim_review/90...  ...     1\n",
              "2356  http://data.gesis.org/claimskg/claim_review/b0...  ...     0\n",
              "7790  http://data.gesis.org/claimskg/claim_review/a4...  ...     1\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#knowledge graph creation\n",
        "# naming entities\n",
        "data['id'] =  data.id \n",
        "data['text'] = data.text \n",
        "data['author'] = data.author \n",
        "data['headline'] = data.headline \n",
        "data['named_entities_claim'] = data.named_entities_claim  \n",
        "data['named_entities_article'] =  data.named_entities_article \n",
        "data['keywords'] =  data.keywords \n",
        "data['source'] = data.source \n",
        "data['language'] = data.language \n",
        "#data['label'] = data.label"
      ],
      "metadata": {
        "id": "YB9WZUBF1MAW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "triples = []\n",
        "for _, row in data.iterrows():\n",
        "    text = (row['id'], \"hasText\", row['text'])\n",
        "    author = (row['id'], \"isWrittenBy\", row['author'])\n",
        "    headline = (row['id'], \"hasHeadline\", row['headline'])\n",
        "    named_entities_claim = (row['id'], \"referredEntities\", row[\"named_entities_claim\"])\n",
        "    keywords = (row['id'], \"containKeywords\", row['keywords'])\n",
        "    language = (row['id'], \"isWrittenIn\", row['language'])\n",
        "    source = (row['id'], \"isLocated\", row['source'])\n",
        "    #label = (row['id'], \"isLabelled\", row['label'])\n",
        "\n",
        "    triples.extend((text, author, headline, named_entities_claim, keywords, language, source))"
      ],
      "metadata": {
        "id": "7idfrZtv1OL0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split triples to train-test set\n",
        "\n",
        "X_train, X_test = train_test_split_no_unseen(np.array(triples), test_size=10000)\n",
        "print('Train set size: ', X_train.shape)\n",
        "print('Test set size: ', X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJMj-z5M1SOk",
        "outputId": "ab39ca8f-ffa6-4972-80e6-12d74ef543b9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size:  (91178, 3)\n",
            "Test set size:  (10000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransE(k=150,                                                             # embedding size\n",
        "               epochs=100,                                                        # Num of epochs\n",
        "               batches_count= 10,                                                 # Number of batches \n",
        "               eta=1,                                                             # number of corruptions to generate during training\n",
        "               loss='pairwise', loss_params={'margin': 1},                        # loss type and it's hyperparameters         \n",
        "               initializer='xavier', initializer_params={'uniform': False},       # initializer type and it's hyperparameters\n",
        "               regularizer='LP', regularizer_params= {'lambda': 0.001, 'p': 3},   # regularizer along with its hyperparameters\n",
        "               optimizer= 'adam', optimizer_params= {'lr': 0.001},                # optimizer to use along with its hyperparameters\n",
        "               seed= 0, verbose=True)\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "model.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT9PwoJ01U-m",
        "outputId": "fd7306ac-9ad0-41ab-aef7-56208f41f71b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Average TransE Loss:   0.024256: 100%|██████████| 100/100 [03:18<00:00,  1.99s/epoch]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating knowledge embeddings\n",
        "print('Size of X_test:', X_test.shape)\n",
        "\n",
        "# Append the Train, Valid and Test set to the X_filter.\n",
        "''' The model has not \"observed\" them during training. We do so because, we would like\n",
        "   to evaluate a test triple against it's corruptions and not against known facts. \n",
        "   If we know that the Validation triples and Test triples are facts (and not queries),\n",
        "   we need to filter these triples out of the generated corruptions. This is the standard \n",
        "   procedure that is used to compute the metrics to compete on the leadership board.'''\n",
        "\n",
        "X_filter = np.concatenate([X_train, X_test], 0)\n",
        "\n",
        "ranks = evaluate_performance(X_test, \n",
        "                             model=model,\n",
        "                             filter_triples=X_filter)\n",
        "\n",
        "mr = mr_score(ranks)\n",
        "mrr = mrr_score(ranks)\n",
        "\n",
        "print(\"MRR: %.2f\" % (mrr))\n",
        "print(\"MR: %.2f\" % (mr))\n",
        "\n",
        "hits_10 = hits_at_n_score(ranks, n=10)\n",
        "print(\"Hits@10: %.2f\" % (hits_10))\n",
        "hits_3 = hits_at_n_score(ranks, n=3)\n",
        "print(\"Hits@3: %.2f\" % (hits_3))\n",
        "hits_1 = hits_at_n_score(ranks, n=1)\n",
        "print(\"Hits@1: %.2f\" % (hits_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmgdhUN71ZEJ",
        "outputId": "f8abbfd5-dfe5-4cd2-e1a6-bc8ce690a2dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X_test: (10000, 3)\n",
            "WARNING - You are attempting to use 66055 distinct entities to generate synthetic negatives in the evaluation\n",
            "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
            "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
            "    of entities. The size of corruption_entities depends on your domain-specific task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ampligraph/evaluation/protocol.py:952: UserWarning: You are attempting to use 66055 distinct entities to generate synthetic negatives in the evaluation\n",
            "    protocol. This may be unnecessary and will lead to a 'harder' task. Besides, it will lead to a much slower\n",
            "    evaluation procedure. We recommended to set the 'corruption_entities' argument to a reasonably sized set\n",
            "    of entities. The size of corruption_entities depends on your domain-specific task.\n",
            "  warnings.warn(warn_msg % ent_for_corruption_size)\n",
            "100%|██████████| 10000/10000 [19:52<00:00,  8.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRR: 0.35\n",
            "MR: 2330.76\n",
            "Hits@10: 0.43\n",
            "Hits@3: 0.38\n",
            "Hits@1: 0.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = data['label']\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(data['id'], y, test_size=0.20, random_state=42)\n",
        "\n",
        "print('X1_train:', len(X1_train))\n",
        "print('y1_train:', len(y1_train))\n",
        "print('****')\n",
        "print('X1_test:', len(X1_test))\n",
        "print('y1_train:', len(y1_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tYjfYnB1bXq",
        "outputId": "2117254e-f2ed-493d-94f0-fa08581c2cbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X1_train: 11563\n",
            "y1_train: 11563\n",
            "****\n",
            "X1_test: 2891\n",
            "y1_train: 2891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the embeddings for X1_train\n",
        "X_train_clf = model.get_embeddings(X1_train)\n",
        "X_test_clf = model.get_embeddings(X1_test)"
      ],
      "metadata": {
        "id": "iVdsGnXG1dXx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XGB classifier\n",
        "clf_model = XGBClassifier(n_estimators=500, max_depth=5, binary=\"logistic\")\n",
        "\n",
        "clf_model.fit(X_train_clf,y1_train)\n",
        "\n",
        "print('Classification report for TransE - XGBclassifier without label information')\n",
        "print(classification_report(y1_test,  clf_model.predict(X_test_clf)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJpjaLBS1fTc",
        "outputId": "7736d97f-0e80-4cf1-9d07-e9bf29f8e8b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification report for TransE - XGBclassifier without label information\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.46      0.14      0.21       904\n",
            "           1       0.70      0.93      0.80      1987\n",
            "\n",
            "    accuracy                           0.68      2891\n",
            "   macro avg       0.58      0.53      0.51      2891\n",
            "weighted avg       0.63      0.68      0.62      2891\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_tensorboard_visualizations(model, 'fn_embeddings_transeNo')"
      ],
      "metadata": {
        "id": "q2Nj6keF8b1w"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}